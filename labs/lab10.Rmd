---
title: "Lab 10"
author: "Mohammed Z Hasan"
output: pdf_document
---

#YARF

For the next couple of labs, I want you to make some use of a package I wrote that offers convenient and flexible tree-building and random forest-building. Make sure you have a JDK installed first

https://www.oracle.com/java/technologies/downloads/

Then try to install rJava

```{r}
options(java.parameters = "-Xmx8000m")
pacman::p_load(rJava)
.jinit()
```

If you have error, messages, try to google them. Everyone has trouble with rJava!

If that worked, please try to run the following which will install YARF from my github:

```{r}
if (!pacman::p_isinstalled(YARF)){
  pacman::p_install_gh("kapelner/YARF/YARFJARs", ref = "dev")
  pacman::p_install_gh("kapelner/YARF/YARF", ref = "dev", force = TRUE)
}
pacman::p_load(YARF)
```

Please try to fix the error messages (if they exist) as best as you can. I can help on slack.


# Missing Data

Load up the Boston Housing Data and separate into matrix `X` for the features and vector `y` for the response. Randomize the rows

```{r}
rm(list = ls())
set.seed(1)
boston = MASS::Boston
boston_shuffled = boston[sample(1 : nrow(boston)), ]
X = as.matrix(boston_shuffled[, 1 : 13])
y = boston_shuffled$medv
rm(boston, boston_shuffled)
```



Similar to lab 1, write a function that takes a matrix and punches holes (i.e. sets entries equal to `NA`) randomly with an argument `prob_missing`.

```{r}
punch_holes = function(mat, prob_missing){
  n = nrow(mat) * ncol(mat)
  is_missing = as.logical(rbinom(n, 1, prob_missing))
  mat[is_missing] = NA
  mat
}
```

Create a matrix `Xmiss` which is `X` but has missingness with probability of 10% using the function you just wrote. 

```{r}
Xmiss = punch_holes(X, .1)
```

What type of missing data mechanism created the missingness in `Xmiss`?

it will be MCAR

Also, generate the M matrix and delete columns that have no missingness.

```{r}
M = apply(is.na(Xmiss), 2, as.numeric)
colnames(M) = paste("is_missing_", colnames(X), sep = "")
M = M[, colSums(M) > 0]
```

Split the first 400 observations were the training data and the remaining observations are the test set. For Xmiss, cbind on the M so the model has a chance to fit on "is missing" as we discussed in class.

```{r}
train_idx = 1 : 400
test_idx = setdiff(1 : nrow(X), train_idx)
X_train =     X[train_idx, ]
Xmiss_train = cbind(Xmiss, M)[train_idx, ]
y_train =     y[train_idx]
X_test =      X[test_idx, ]
Xmiss_test =  cbind(Xmiss, M)[test_idx, ]
y_test =      y[test_idx]
```

Fit a random forest model of `y_train ~ X_train`, report oos s_e (not oob) on `X_test`. This ignores missingness

```{r}
#we are fitting a random forest model here
model_rf = YARF(data.frame(X_train), y_train) #using profs YARF to create the model
y_hat_test = predict(model_rf, data.frame(X_test))
sqrt(mean((y_hat_test - y_test)^2))
```

Impute the missingness in `Xmiss` using the feature averages to create a matrix `Ximp_naive_train` and `Ximp_naive_test`. 

```{r}
Ximp_naive_test = Xmiss_train
Ximp_naive_test = Xmiss_test
x_averages = array(NA, ncol(X))
for (j in 1: ncol(X)) {
  x_averages[j] = mean(Xmiss_train, na.rm = TRUE)
  Ximp_naive_train[is.na(Xmiss_train[, j]), j] = x_averages[j]
  Ximp_naive_test[is.na(Xmiss_test[, j]), j] = x_averages[j]
  
}
```

Fit a random forest model of `y_train ~ Ximp_naive_train`, report oos s_e (not oob) on `Ximp_naive_test`.

```{r}
model_rf = YARF(data.frame(Ximp_naive_train), y_train)
y_hat_test = predict(model_rf, data.frame(Ximp_naive_test))
sqrt(mean((y_hat_test - y_test)^2))
```

How much predictive performance was lost due to missingness when naive imputation was used vs when there was no missingness?

```{r}
model_rf_no_miss = YARF(data.frame(X_train), y_train)
y_hat_test_no_miss = predict(model_rf_no_miss, data.frame(X_test))
rmse_no_miss = sqrt(mean((y_hat_test_no_miss - y_test)^2))

model_rf_naive_impute = YARF(data.frame(Ximp_naive_train), y_train)
y_hat_test_naive_impute = predict(model_rf_naive_impute, data.frame(Ximp_naive_test))
rmse_naive_impute = sqrt(mean((y_hat_test_naive_impute - y_test)^2))

performance_loss = rmse_naive_impute - rmse_no_miss

# Output the RMSE values and the performance loss
rmse_no_miss
rmse_naive_impute
performance_loss

```

Use `missForest` to impute the missing entries to create a matrix `Ximp_MF_train` and `Ximp_MF_test`.

```{r}
pacman::p_load(missForest)
Ximp_MF_train = missForest(Xmiss_train)$ximp
Xymiss = rbind(
  cbind(Xmiss_train, y_train),
  cbind(Xmiss_test, NA)
)
Xyimp_miss = missForest(Xymiss)$ximp
Ximp_MF_train = Xyimp_miss[train_idx, 1 : ncol(X)]
Ximp_MF_test = Xyimp_miss[test_idx, 1 : ncol(X)]
```

Fit a random forest model of `y_train ~ Ximp_MF_train`, report oos s_e (not oob) on `Ximp_MF_test`.

```{r}
model_rf = YARF(data.frame(Ximp_MF_train), y_train)
y_hat_test = predict(model_rf, data.frame(Ximp_MF_test))
sqrt(mean((y_hat_test - y_test)^2))
```

How much predictive performance was lost due to missingness when `missForest` imputation was used?

```{r}
model_rf_no_miss = randomForest(X_train, y_train)
y_hat_test_no_miss = predict(model_rf_no_miss, X_test)
rmse_no_miss = sqrt(mean((y_hat_test_no_miss - y_test)^2))

model_rf_miss_forest = randomForest(Ximp_MF_train, y_train)
y_hat_test_miss_forest = predict(model_rf_miss_forest, Ximp_MF_test)
rmse_miss_forest = sqrt(mean((y_hat_test_miss_forest - y_test)^2))

performance_loss_miss_forest = rmse_miss_forest - rmse_no_miss

# Output the results
rmse_no_miss
rmse_miss_forest
performance_loss_miss_forest

```

Why did `missForest` imputation perform better than naive imputation?

missfoirest works better on datasets with missing values rahther than naive imputation due to it being operated iteratively.

Reload the feature matrix:

```{r}
rm(list = ls())
X = as.matrix(MASS::Boston[, 1 : 13])
```

Create missingness in the feature `lstat` that is due to a MAR missing data mechanism.

```{r}
medv = Boston$medv

prob_missing = pnorm((medv - mean(medv)) / sd(medv), mean = 1, sd = 1)

threshold = 0.75 

set.seed(123)
missing_indices = runif(length(prob_missing)) < prob_missing
X[missing_indices, which(colnames(X) == "lstat")] = NA

head(X)
```

Create missingness in the feature `rm` that is a NMAR missing data mechanism.

```{r}
set.seed(123)

prob_missing_rm = pnorm((X[, "rm"] - mean(X[, "rm"])) / sd(X[, "rm"]), mean = 1, sd = 1)

threshold_rm = 0.75 

missing_indices_rm = runif(nrow(X)) < prob_missing_rm
X[missing_indices_rm, which(colnames(X) == "rm")] = NA

head(X)

```


#Bagged Trees and Random Forest

Take a training sample of n = 2000 observations from the diamonds data.

```{r}
rm(list = ls())
pacman::p_load(tidyverse)
set.seed(1)
diamonds_train = ggplot2::diamonds %>% 
  sample_n(2000)
y_train = diamonds_train
X_train = diamonds
```


Using the diamonds data, find the oob s_e for a bagged-tree model using 1, 2, 5, 10, 20, 30, 40, 50, 100, 200, 300, 400, 500, 1000 trees. If you are using the `randomForest` package, you can create the bagged tree model via setting an argument within the RF constructor function. Plot.

```{r}
num_trees_values = c(1, 2, 5, 10, 20, 30, 40, 50, 100, 200, 300, 400, 500, 1000)
oob_se_bagged_trees_mod_by_num_trees = array(NA, length(num_trees_values))
for (m in 1 : length(num_trees_values)) {
  YARFBAG(X_train, y_train, num_trees)
  oob_se_bagged_trees_mod_by_num_trees
}
```

Find the bootstrap s_e for a RF model using 1, 2, 5, 10, 20, 30, 40, 50, 100, 200, 300, 400, 500, 1000 trees. If you are using the `randomForest` package, you can calculate oob residuals via `e_oob = y_train - rf_mod$predicted`. Plot.

```{r}
oob_se_rf_mod_by_num_trees = array(NA, length(num_trees_values))

library(randomForest)

num_trees_values = c(1, 2, 5, 10, 20, 30, 40, 50, 100, 200, 300, 400, 500, 1000)
oob_se_rf_mod_by_num_trees = numeric(length(num_trees_values))

for (m in seq_along(num_trees_values)) {
  # Fit the Random Forest model
  rf_mod = randomForest(x = X_train, y = y_train, ntree = num_trees_values[m], mtry = floor(sqrt(ncol(X_train))), keep.inbag = TRUE, keep.forest = TRUE)
  oob_predictions = predict(rf_mod, X_train, predict.all = TRUE)$aggregate[rf_mod$inbag == 0]
  actual_values = y_train[rf_mod$inbag == 0]
  oob_residuals = actual_values - oob_predictions

  oob_se_rf_mod_by_num_trees[m] = sd(oob_residuals, na.rm = TRUE)
}

```

What is the percentage gain / loss in performance of the RF model vs bagged trees model for each number of trees? Gains are negative (as in lower oos s_e).

```{r}
cbind(
  num_trees_values,
  (oob_se_rf_mod_by_num_trees - oob_se_bagged_trees_mod_by_num_trees) / oob_se_bagged_trees_mod_by_num_trees * 100
)
```

Why was this the result?

the nature of random forest and the steps used in them are why this is the result

Plot oob s_e by number of trees for both RF and bagged trees by creating a long data frame from the two results.

```{r}
ggplot(results_long, aes(x = num_trees, y = OOB_SE, color = Model, group = Model)) +
  geom_line() +  # Line plot to show trends
  geom_point() +  # Points to highlight actual data points
  labs(title = "OOB Standard Error by Number of Trees",
       x = "Number of Trees",
       y = "OOB Standard Error",
       color = "Model") +
  theme_minimal()
```

Build RF models for 500 trees using different `mtry` values: 1, 2, ... the maximum. That maximum will be the number of features assuming that we do not binarize categorical features if you are using `randomForest` or the number of features assuming binarization of the categorical features if you are using `YARF`. Calculate oob s_e for all mtry values.

```{r}
oob_se_by_mtry = array(NA, ncol(diamonds_train))

library(randomForest)

max_mtry = ncol(X_train)

oob_se_by_mtry = numeric(max_mtry)

for (mtry_val in 1:max_mtry) {

  rf_model = randomForest(x = X_train, y = y_train, ntree = 500, mtry = mtry_val, keep.inbag = TRUE)

  oob_predictions = predict(rf_model, X_train, predict.all = TRUE)$aggregate[rf_model$inbag == 0]
  actual_values = y_train[rf_model$inbag == 0]
  oob_residuals = actual_values - oob_predictions

  oob_se_by_mtry[mtry_val] = sd(oob_residuals, na.rm = TRUE)
}
oob_se_by_mtry

```

Plot oob s_e by mtry.

```{r}
ggplot(mtry_df, aes(x = mtry, y = OOB_SE)) +
  geom_line() +  # Line plot to show the trend
  geom_point(color = 'blue', size = 2) +  # Points to mark each data point
  labs(title = "OOB Standard Error by Mtry",
       x = "Mtry (Number of Features at Each Split)",
       y = "OOB Standard Error") +
  theme_minimal() +
  theme(plot.title = element_text(halign = "center"))
```

Take a sample of n = 2000 observations from the adult data and name it `adult_sample`. Then impute missing values using missForest.

```{r}
rm(list = ls())
set.seed(1)
pacman::p_load_gh("coatless/ucidata")
adult_train = adult %>% 
  sample_n(2000)
adult_train = missForest(adult_train)$ximp
```


Using the adult_train data, find the bootstrap misclassification error for a bagged-tree model using 1, 2, 5, 10, 20, 30, 40, 50, 100, 200, 300, 400, 500, 1000 trees. Plot.

```{r}
num_trees_values = c(1, 2, 5, 10, 20, 30, 40, 50, 100, 200, 300, 400, 500, 1000)
oob_se_bagged_trees_mod_by_num_trees = array(NA, length(num_trees_values))
for (i in seq_along(num_trees_values)) {
  set.seed(1)  
  rf_model = randomForest(x = X_train, y = y_train, ntree = num_trees_values[i], mtry = floor(sqrt(ncol(X_train))), keep.inbag = TRUE, importance = FALSE)
  oob_error[i] = rf_model$err.rate[ntree(rf_model), "OOB"]
}

results_df = data.frame(Trees = num_trees_values, OOB_Error = oob_error)

ggplot(results_df, aes(x = Trees, y = OOB_Error)) +
  geom_line() +
  geom_point() +
  labs(title = "OOB Misclassification Error by Number of Trees",
       x = "Number of Trees",
       y = "OOB Misclassification Error") +
  theme_minimal()
```

Using the adult_train data, find the bootstrap misclassification error for an RF model using 1, 2, 5, 10, 20, 30, 40, 50, 100, 200, 300, 400, 500, 1000 trees.

```{r}
oob_se_rf_mod_by_num_trees = array(NA, length(num_trees_values))
for (i in seq_along(num_trees_values)) {
  set.seed(1)  # For reproducibility
  rf_model = randomForest(x = X_train, y = y_train, ntree = num_trees_values[i], keep.inbag = TRUE)
  oob_error_rf[i] = rf_model$err.rate[ntree(rf_model), "OOB"]
}

results_df_rf = data.frame(Trees = num_trees_values, OOB_Error = oob_error_rf)

ggplot(results_df_rf, aes(x = Trees, y = OOB_Error)) +
  geom_line() +
  geom_point() +
  labs(title = "OOB Misclassification Error by Number of Trees in RF",
       x = "Number of Trees",
       y = "OOB Misclassification Error") +
  theme_minimal()
```

What is the percentage gain / loss in performance of the RF model vs bagged trees model?

```{r}
cbind(
  num_trees_values,
  (oob_se_rf_mod_by_num_trees - oob_se_bagged_trees_mod_by_num_trees) / oob_se_bagged_trees_mod_by_num_trees * 100
)
```

Build RF models on adult_train for 500 trees using different `mtry` values: 1, 2, ... the maximum (see above as maximum is defined by the specific RF algorithm implementation). 

```{r}
oob_se_by_mtry = array(NA, ncol(adult_train))
for (mtry_val in 1:max_mtry) {
  set.seed(123)  # Ensure reproducibility
  rf_model = randomForest(x = X_train, y = y_train, ntree = 500, mtry = mtry_val, keep.inbag = TRUE)

  oob_se_by_mtry[mtry_val] = rf_model$err.rate[ntree(rf_model), "OOB"]
}
```


Plot bootstrap misclassification error by `mtry`.

```{r}
library(ggplot2)

mtry_values = 1:length(oob_se_by_mtry) 
error_data = data.frame(mtry = mtry_values, OOB_Error = oob_se_by_mtry)

ggplot(error_data, aes(x = mtry, y = OOB_Error)) +
  geom_line() +  # Line to show trend
  geom_point(aes(color = mtry), size = 2) +  # Points colored by mtry value
  scale_color_gradient(low = "blue", high = "red") +  # Color gradient for visual aid
  labs(title = "Bootstrap Misclassification Error by Mtry",
       x = "Mtry (Number of Features at Each Split)",
       y = "OOB Misclassification Error") +
  theme_minimal() +
  theme(plot.title = element_text(halign = "center"))
```

Is `mtry` an important hyperparameter to optimize when using the RF algorithm? Explain

Yes, it represents the number of features to consider at each split when building each tree within the forest.

Identify the best model among all values of `mtry`. Fit this RF model. Then report the following oob error metrics: misclassification error, precision, recall, F1, FDR, FOR and compute a confusion matrix.

```{r}
y_pred = predict(optimal_rf_model, type = "response")

conf_mat = confusionMatrix(y_pred, y_train)

oob_misclassification_error = 1 - conf_mat$overall['Accuracy']
oob_precision = conf_mat$byClass['Precision']
oob_recall = conf_mat$byClass['Recall']
oob_F1 = conf_mat$byClass['F1']
oob_FDR = 1 - oob_precision 
oob_FOR = 1 - conf_mat$byClass['Negative Predictive Value'] 

optimal_mtry
oob_misclassification_error
oob_precision
oob_recall
oob_F1
oob_FDR
oob_FOR

```

Is this a good model? (yes/no and explain).

it really depends on the in impact of false positives and false negatives. But in general yes it is a good model.

There are probability asymmetric costs to the two types of errors. Assign two costs below and calculate oob total cost.

```{r}
fp_cost = 5
fn_cost = 10

y_pred = predict(optimal_rf_model, type = "response")

conf_mat = confusionMatrix(y_pred, y_train)

fp_count = conf_mat$table["No", "Yes"]  
fn_count = conf_mat$table["Yes", "No"]

oob_total_cost = (fp_cost * fp_count) + (fn_cost * fn_count)

oob_total_cost
```

# Asymmetric Cost Modeling, ROC and DET curves

Fit a logistic regression model to the adult_train missingness-imputed data.

```{r}
rm(list = setdiff(ls(), "adult_train"))
library(dplyr)

adult_train$income = as.factor(adult_train$income)

response_var = "income" 
predictors = setdiff(names(adult_train), response_var)
formula = as.formula(paste(response_var, "~", paste(predictors, collapse = "+")))

logistic_model = glm(formula, data = adult_train, family = binomial())

summary(logistic_model)
```

Use the function from class to calculate all the error metrics (misclassification error, precision, recall, F1, FDR, FOR) for the values of the probability threshold being 0.001, 0.002, ..., 0.999 in a tibble (dplyr data frame).

```{r}
pacman::p_load(tidyverse)
asymmetric_predictions_results = tibble(
  p_hat_threshold = seq(from = 0.001, to = 0.999, by = 0.001),
  misclassification_error = NA, 
  precision = NA, 
  recall = NA, 
  F1 = NA, 
  FDR = NA, 
  FOR = NA
)
#TO-DO
```

Calculate the column `total_cost` and append it to this data frame via `mutate`.

```{r}
asymmetric_predictions_results = asymmetric_predictions_results %>%
  mutate(
    fp_fn_counts = map(p_hat_threshold, calculate_fp_fn_counts, data = extended_data),
    fp_count = map_dbl(fp_fn_counts, 1),
    fn_count = map_dbl(fp_fn_counts, 2),
    total_cost = (fp_count * fp_cost) + (fn_count * fn_cost)
  ) %>%
  select(-fp_fn_counts) 

asymmetric_predictions_results

```

Which is the lowest total cost? What is the "winning" probability threshold value providing that minimum total cost?

```{r}
min_total_cost = asymmetric_predictions_results %>%
  summarize(min_cost = min(total_cost)) %>%
  pull(min_cost)

winning_threshold = asymmetric_predictions_results %>%
  filter(total_cost == min_total_cost) %>%
  select(p_hat_threshold, total_cost) %>%
  slice(1) 

winning_threshold
```

Plot an ROC curve and interpret.

```{r}
probabilities = predict(logistic_model, adult_train, type = "response")

roc_data = response = adult_train$response_variable, predictor = probabilities

roc_curve_plot = ggplot(data = data.frame(
                            False_Positive_Rate = roc_data$specificities, 
                            True_Positive_Rate = roc_data$sensitivities),
                         aes(x = False_Positive_Rate, y = True_Positive_Rate)) +
    geom_line(color = "blue") +
    geom_abline(linetype = "dashed") +
    labs(title = "ROC Curve", x = "False Positive Rate (1 - Specificity)", y = "True Positive Rate (Sensitivity)") +
    theme_minimal()
```

